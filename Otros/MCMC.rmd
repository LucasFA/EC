---
title: "Introducción a MCMC"
author: "Lucas Fehlau Arbulu"
date: "06-06-2022"
output: 
    html_document: 
        toc: true
        number_sections: false
        toc_float: true
---

# Introducción

Vamos a ver qué son los Métodos de Montecarlo basados en cadenas de Markov.
Las siglas vienen del inglés, *Markov Chain Monte Carlo*
<!-- TODO: cadenas? -->

## Métodos de Montecarlo

Los métodos de Montecarlo en general pretenden utilizar muestreo aleatorio repetidamente
para obtener algún tipo de resultado numérico.

Por ejemplo, aquí incluyo unos scripts de R para aproximar el valor de la siguiente integral.

$$
    \int_0^5 \sin(x) e^{-x} dx
$$

```{r setup, echo = FALSE}
set.seed(1)
```

```{r}
f <- function(x) sin(x) * exp(-x)
curve(f, 0, 5)
```

Para ello lo que hacemos es tomar un número de muestras de una variable $U(0,5)$ del espacio y evaluamos $f$ en esos puntos.

```{r}
numero_simulaciones <- 1000
x <- runif(numero_simulaciones, min = 0, max = 5)
fx <- f(x)
```
Entonces el valor aproximado de la integral es
```{r}
integral_aprox <- (5 - 0) * mean(fx)
integral_aprox
```

Que intuitivamente puedes llegar a reducirlo, a base por altura media.
El valor aproximado por un método numérico clásico sería

```{r, echo = FALSE}
integral <- integrate(f, 0, 5)$value
integral
```

Además podemos ver cómo evoluciona la aproximación de acuerdo al número de muestras aleatorias tomadas:

```{r, echo = FALSE}
estim <- 5 * (cumsum(fx) / seq_along(fx))
# errores de estimación correspondientes
estim.err <- sqrt(cumsum((fx - estim)^2)) / seq_along(fx)
plot(1:numero_simulaciones, estim,
    type = "l", ylab = "Aproximación y límites de error",
    xlab = "Número de simulaciones",
    main = expression("Aproximación de la integral"),
    ylim = c(0.2, 1)
)
z <- qnorm(0.025, lower.tail = FALSE)
# lines(estim - z * estim.err, col = "blue", lwd = 2, lty = 3)
# lines(estim + z * estim.err, col = "blue", lwd = 2, lty = 3)
abline(h = integral, lwd = 0.5, lty = "dotted")
```
Aquí además hemos indicado en horizontal el valor de la integral calculada numéricamente.

Montecarlo también se puede utilizar para aproximar la probabilidad de un suceso o, como es más utilizado,
para aproximar distribuciones de probabilidad cuya expresión analítica

# Cadenas de Markov en métodos de Montecarlo

Aquí vamos a abordar el caso de que queramos tomar muestras de una distribución determinada, 
con motivos de simulación, por ejemplo.

## Idea

Lo que vamos a hacer es crear una cadena de Markov que tenga como distribución límite
la distribución deseada, para lo cual debemos exigir que cumpla las condiciones del teorema ergódico,
de forma que la distribución límite sea la estacionaria. 

Así, vamos a buscar una cadena de Markov irreducible, aperiódica en la cual
todos los estados sean recurrentes positivos.

Entonces, dada esta cadena, vamos a caminar por la cadena, y el estado 
resultante de cada paso es una muestra de la distribución deseada.
<!-- 
### Requisitos

Sea $\{X_t\}_{t\in \mathbb{R}}$ una cadena de Markov. Recordamos que
si $a^{(n)}$ es la distribución absoluta y $\boldsymbol{P}$ es la matriz de transición,
entonces $a^{(n+1)} = a^{(n)} \boldsymbol{P}$. 
-->


## Metrópolis

Sea $f$ una función proporcional a la distribución que deseamos aproximar.
Sea $q(x|y)$ una distribución de probabilidad simétrica que usaremos para generar un nuevo candidato
dado el punto actual $y$. Típicamente se elige la distribución normal (en el número de dimensiones adecuado).
Se toma un estado $x_i$ arbitrario.

Entonces el método consiste en:

1. Se propone un nuevo estado $x'$ a partir de $x_i$ con $Q$
2. Se calcula la probabilidad de aceptación del candidato $\alpha = min\{1, f(x')/f(x)\}$
3. Aceptar la nueva posición con probabilidad $\alpha$ (tomando una muestra de $U(0,1$)).
4. El siguiente estado es o bien el nuevo, aceptado, o nos mantenemos en el previo, si fue rechazado.

Se obtiene así una sucesión $\{x_n\}_{n\in\mathbb{N}}$ de muestras dependientes.
Esta dependencia perderá relevancia para un número suficiente de pasos.

Intuitivamente, los estados más visitados son los más probables de acuerdo con $f$.

### Un ejemplo

Tomaremos $X$ una variable aleatoria una mixtura[^Mixtura] de dos distribuciones normales. Este tipo de distribución
puede resultar de tomar muestras de una mezcla heterogénea respecto a alguna característica.

[^Mixtura]: Una mixtura de una familia de variables aleatorias $\{X_i\}_{i\in\Gamma}$ 
es una variable aleatoria cuya distribución es una suma convexa de las funciones de 
distribución de las $\{X_i\}_{i\in\Gamma}$. Ejemplo: si $X_1 \sim U(0,1)$ y $X_2 \sim U(2, 4)$, 
una mixtura sería por ejemplo $X\sim f_X$, donde 
$f_X(x) = \frac{1}{2} f_{X_1}(x) + \frac{1}{2} f_{X_2}(x)$ 
```{r, echo= FALSE} 
f <- function(x) (0 <= x & x <= 1) * 1 / 2 + (2<= x & x <= 4) * 1/4
plot(f, from = -1, to = 5, n = 200)
```


```{r}
p <- 0.4
medias <- c(-1, 2)
desv_tipicas <- c(.5, 2)
f <- function(x) {
    p * dnorm(x, medias[1], desv_tipicas[1]) +
    (1 - p) * dnorm(x, medias[2], desv_tipicas[2])
}
curve(f(x), col = "red", -4, 8, las = 1)
```

```{r}
# Generará los nuevos posibles estados.
q <- function(x) rnorm(n = 1, mean = x, sd = 4)

# Da un paso del método anteriormente descrito
paso <- function(punto_previo, f, q) {
    ## Nuevo posible punto:
    candidato <- q(punto_previo)
    ## Probabilidad de aceptar el nuevo punto:
    alpha <- min(1, f(candidato) / f(punto_previo))
    ## Aceptarlo con probabilidad alpha
    if (runif(1) < alpha) {
        candidato
    } else { # de lo contrario, quedarse quieto
        punto_previo
    }
}

# Ejecuta el número de pasos indicado
método <- function(x, f, q, npasos) {
    res <- matrix(NA, npasos, length(x))
    for (i in seq_len(npasos))
        res[i, ] <- x <- paso(x, f, q)
    drop(res)
}
```

Nota: aquí, `método` está implementado de esta extraña manera porque permitirá generalizar a 
el caso multidimensional, los cuales son los interesantes.

Empezando en un punto arbitrario, por ejemplo, $20$:
```{r}
res <- método(20, f, q, 1000)
```

```{r, echo = FALSE}
layout(matrix(c(1, 2), 1, 2), widths = c(4, 1))
par(mar = c(4.1, .5, .5, .5), oma = c(0, 4.1, 0, 0))
plot(res, type = "s", xpd = NA, ylab = "Parameter", xlab = "Sample", las = 1)
usr <- par("usr")
xx <- seq(usr[3], usr[4], length = 301)
plot(f(xx), xx, type = "l", yaxs = "i", axes = FALSE, xlab = "")
```

```{r, echo = FALSE}
hist(res, 50,
    freq = FALSE, main = "", ylim = c(0, .4), las = 1,
    xlab = "x", ylab = "Probability density"
)
z <- integrate(f, -Inf, Inf)$value
dist_real <- function(x) f(x)/z
curve(dist_real(x), add = TRUE, col = "red")
```

Para $n$ más grande:

```{r}
set.seed(1)
res.largo <- método(-10, f, q, 50000)
hist(res.largo, 100,
    freq = FALSE, main = "", ylim = c(0, .4), las = 1,
    xlab = "x", ylab = "Probability density", col = "grey"
)
curve(dist_real(x), add = TRUE, col = "red")
```
